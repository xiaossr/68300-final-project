<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Quantifying Spatial Locality in Vision Transformers</title>
  <link rel="stylesheet" href="styles.css">

  <!-- MathJax -->
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['\\(','\\)']],
        displayMath: [['$$','$$'], ['\\[','\\]']]
      }
    };
  </script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>
  <article>
    <h1>Quantifying Spatial Locality in Vision Transformers</h1>

    <section id="introduction">
      <h2>1. Introduction and Motivation</h2>
      <p>
        Vision Transformers (ViTs) have revolutionized image recognition by replacing convolutional operations with global self-attention, achieving top-tier performance on benchmarks like CIFAR-10. Unlike convolutional neural networks (CNNs), which build in local receptive fields by design, ViTs operate over sequences of image patches with no explicit bias toward nearby pixels. Yet practitioners and theorists alike have noted that ViTs often display strong local behavior, especially in early layers, hinting at an emergent spatial inductive bias.      </p>
      <p>
        Understanding how ViTs balance global context against local structure is critical for interpretability and for guiding architectural improvements. If local interactions dominate early layers, one might imagine hybrid models that explicitly combine convolutional kernels with self-attention. Conversely, if global heads already capture most of the information, simpler Transformer-only designs may suffice. Our project explores this tension by systematically perturbing token representations at various spatial distances and measuring the effect on downstream predictions. By ablating 'local' versus 'distant' tokens in a controlled manner, we aim to quantify the true importance of spatial locality in each attention head and layer.      </p>
      <p>
        <em>Note:</em> We use CIFAR-10 and the ViT-base-patch-32 model due to limited available compute. This setup serves as a quick prototype and benchmarking platform before scaling to larger datasets or models.
      </p>
    </section>

    <section id="background">
      <h2>2. Background and Related Work</h2>
      <p>
        The original ViT formulation by Dosovitskiy et al. (2020) demonstrated that pure Transformer architectures - when trained on large-scale datasets - can match or surpass CNN performance on image classification tasks [1]. Subsequent variants, such as DeiT (Touvron et al., 2021), improved data efficiency through distillation techniques, but retained the global self-attention mechanism.      </p>
      <p>
        <strong>CLIP</strong> (Contrastive Language-Image Pre-training) by Radford et al. (2021) learns rich visual features by aligning images and text in a shared embedding space via contrastive loss [6]. Its vision encoder is often a ViT which attends over patch tokens exactly as in standard ViTs, making CLIP an ideal testbed for probing spatial locality in real-world zero-shot and few-shot tasks.
      </p>
      <p>
        Caron et al. (2023) studied patch-level attention patterns, finding that certain heads specialize in local edge detection while others capture long-range dependencies [2]. Kazemied et al. (2024) extended this by evaluating how token information is retained across layers, suggesting that intermediate layers preserve more spatial structure than very deep or very shallow ones [3]. However, both studies relied primarily on attention-weight visualizations or information-theoretic measures rather than direct perturbation experiments.      </p>
      <p>
        The LessWrong community's work on multimodal audio interpretability uncovered that - even without spatial architectures - attention mechanisms can develop localized grouping behaviors when tasked with spectrogram representations [4]. These findings inspired our hypothesis that ViTs (and CLIP's ViT backbone) may likewise reduce to local processing under certain conditions, despite their global connectivity.      </p>
      <p>
        <strong>Literature Gap.</strong> While prior work has catalogued attention weight patterns [2], information retention [3], analogies in audio models [4], and demonstrated CLIP's generality [6], there remains a lack of empirical studies that directly compare the impact of local versus distant token perturbations on model outputs. No published work has systematically ablated tokens at varying spatial distances and quantified the resulting performance degradation across layers and heads. Our project fills this gap with a perturbation-based framework to measure spatial locality in ViTs at scale.
      </p>
    </section>

    <section id="methodology">
      <h2>3. Methodology</h2>

      <h3>3.1 Dataset and Model</h3>
      <p>
        We use CIFAR-10 exclusively to enable rapid prototyping on limited compute. We load CLIP's ViT backbone as follows:
      </p>
      <pre><code>model_name = "openai/clip-vit-base-patch32"
vision = CLIPVisionModelWithProjection.from_pretrained(
   model_name,
   output_attentions=True,     # turn on attention outputs
   attn_implementation="eager" # silence the manual-fallback warning
)</code></pre>
      <p>
        This lightweight setup provides a benchmark for quicker iteration before scaling to larger datasets or model variants.
      </p>

      <h3>3.2 Why Attention Matrices?</h3>
      <p>
        Attention matrices are our empirical window into how each layer and head routes spatial information. They tell us where the model is focusing and how far it's willing to look - critical data for testing token-level spatial locality in CLIP. By inspecting these matrices directly, we can identify heads with local versus global bias before applying targeted interventions.
      </p>

      <h3>3.3 Token-Distance Bias</h3>
      <p>
        We quantify token-distance bias via the decay of attention weight versus Euclidean patch distance:
      </p>
      <ul>
        <li><strong>Sharper decay ⇒ stronger locality.</strong> If a layer's curve drops quickly (high weight at \(\,d=0,1\)\, but near zero for \(d>2\)), its heads focus almost exclusively on nearby patches.</li>
        <li><strong>Layerwise evolution.</strong> Early layers may be highly local (steep decay); later layers flatten out as global context is incorporated.</li>
        <li><strong>Guides further experiments.</strong> Knowing which layers are local or global informs targeted activation-patching or masking for causal tests.</li>
      </ul>
      <p>
        This baseline uses Euclidean distances to explain spatial decay, but our head rankings for ablation rely on "distance" as measured by the attention matrix itself (mean attention-weighted separation), not strictly geometric separation.
      </p>

      <h3>3.4 Attention Formulation and Computational Pipeline</h3>
      <p>
        We adopt standard scaled dot-product self-attention. For layer \(\,l\)\ and head \(\,h\):
      </p>
      <div class="formula">
        $$\alpha^{(l,h)} = \mathrm{softmax}\Bigl(\tfrac{Q^{(l,h)}K^{(l,h)\,T}}{\sqrt{d_k}}\Bigr)
          \in \mathbb{R}^{(N+1)\times(N+1)}.$$
      </div>
      <p>
        Dropping the [CLS] token and averaging over a batch yields patch-to-patch attention maps. We compute an Euclidean distance matrix \(\,D_{i,j}\)\ to define masking radii precisely, and separately derive head "distances" from the attention matrices when ranking heads for ablation.
      </p>

      <h3>3.5 Attention Rollout</h3>
      <p>
        Using the attention-rollout method (Abnar & Zuidema, 2020) [7], we back-track from CLS to input patches:
      </p>
      <div class="formula">
        $$R = \prod_{l=1}^k (\alpha^{(l)} + I),$$
      </div>
      <p>
        where \(I\) adds residual identity. The resulting heatmap \(R\) shows each patch's influence on the final representation.
      </p>

      <h3>3.6 Attention Masking</h3>
      <p>
        We apply a distance-based binary mask \(M^{(r)}\in\{0,-\infty\}^{N\times N}\) to constrain attention to within radius \(r\):
      </p>
      <div class="formula">
        $$M^{(r)}_{i,j} = \begin{cases}
          0, & D_{i,j} \le r,\\
          -\infty, & D_{i,j} > r,
        \end{cases}
        \quad
        \alpha^{(l,h;r)} = \mathrm{softmax}\Bigl(\tfrac{QK^T}{\sqrt{d_k}} + M^{(r)}\Bigr).$$
      </div>
      <p>
        Sweeping \(r\) from strictly local to full image diagonal, we measure zero-shot CLIP accuracy under each constraint to isolate token-distance contributions.
      </p>

      <h3>3.7 Token Ablation</h3>
      <p>
        We rank heads by their mean attention-based "distance" (mean token separation as seen in their attention maps) and ablate nearest or farthest tokens for the \(k\) most local or global heads (\(k=1,3,\dots,25\)). Measuring accuracy drops on CIFAR-10 assesses the causal importance of spatially proximate tokens, complementing masking experiments.
      </p>
    </section>

    <section id="results">
      <h2>4. Results</h2>

      <h3>4.1 Strong Spatial Locality Across Patches</h3>
      <p>
        Across all layers and heads, we observe steep attention-weight decay with distance, confirming strong spatial locality throughout the model. Early layers show the sharpest drops, while deeper layers gradually flatten, indicating a progressive incorporation of global context.
      </p>
      <img src="figures/token_distance_bias_CLIP.png" alt="Token-distance bias curves for each layer in CLIP's ViT">

      <p>The table below shows, for each layer, the single most 'local' head (smallest mean Euclidean distance) and the single most 'global' head (largest mean Euclidean distance):</p>
      <table>
        <thead>
          <tr>
            <th>Layer</th>
            <th>Local Head Index</th>
            <th>Local Mean Distance</th>
            <th>Global Head Index</th>
            <th>Global Mean Distance</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>0</td><td>8</td><td>0.9144</td><td>3</td><td>3.7583</td></tr>
          <tr><td>1</td><td>11</td><td>1.1327</td><td>9</td><td>3.3411</td></tr>
          <tr><td>2</td><td>3</td><td>1.1799</td><td>8</td><td>3.6848</td></tr>
          <tr><td>3</td><td>5</td><td>1.2347</td><td>10</td><td>3.8939</td></tr>
          <tr><td>4</td><td>4</td><td>1.2922</td><td>2</td><td>3.4153</td></tr>
          <tr><td>5</td><td>5</td><td>1.9311</td><td>1</td><td>3.6301</td></tr>
          <tr><td>6</td><td>10</td><td>2.0669</td><td>0</td><td>3.5240</td></tr>
          <tr><td>7</td><td>10</td><td>0.8289</td><td>11</td><td>4.0369</td></tr>
          <tr><td>8</td><td>1</td><td>2.7692</td><td>11</td><td>3.6648</td></tr>
          <tr><td>9</td><td>10</td><td>2.9206</td><td>0</td><td>3.4444</td></tr>
          <tr><td>10</td><td>1</td><td>3.0504</td><td>4</td><td>3.5865</td></tr>
          <tr><td>11</td><td>1</td><td>3.3551</td><td>8</td><td>3.6279</td></tr>
        </tbody>
      </table>
      <p>
        In the earliest layers (0-3), the most local heads have very low mean distances (around 0.9-1.2), indicating tight focus on immediate neighbors, while the most global heads range around 3.3-3.9, capturing far-flung tokens. By layer 7, the single most local head drops to 0.8289, showing extreme locality, and the most global head peaks at 4.0369. Deeper layers (8-11) see local heads’ mean distances increase (up to ~3.35), suggesting that even 'local' heads start to integrate broader context as depth grows. Global-head distances remain relatively stable (~3.4-3.9), implying a consistent far-range role across the network.
      </p>
      <p>
        Overall, early layers specialize in extracting fine-grained, local features, while later layers gradually blend in more global context. For CLIP and similar ViT-based models, this pattern suggests a two-stage processing: first, capture local texture and edges; then, integrate them into a holistic image understanding. Understanding these head-wise roles can guide targeted interventions (e.g., freezing or pruning specific heads) and inspire hybrid architectures that explicitly allocate local vs. global processing capacities.
      </p>

      <h3>4.2 Attention Radius Constraint</h3>
      <p>
        The plot below shows zero-shot CIFAR-10 accuracy as we vary the Euclidean attention radius 
        (in patch units) in CLIP ViT-base-patch-32:
      </p>
      <img src="figures/euclidean_attention_radius_CLIP.png" alt="Attention radius vs accuracy">
      <p>
        Accuracy rises from ~20% at k=0 to ~81% at k=3, then plateaus near baseline (~89%) by k=6.  
      </p>
      <p>
        <strong>Interpretation:</strong>  Most of CLIP's CIFAR-10 performance can be captured by very local interactions (within 3-patch radius). Beyond that, distant tokens contribute marginally, reinforcing that spatial locality underlies the model's learned representations.
      </p>

      <h3>4.3 Token Ablation with k Local/Global Heads</h3>
      <p>
        The solid blue curve shows that ablating local heads provokes a steep performance drop: accuracy declines from approximately 0.89 at \(k=0\) to 0.80 at \(k=5\), to 0.69 at \(k=10\), and ultimately to under 0.55 by \(k=20\). In contrast, the solid orange curve for global-head ablation remains relatively flat, decreasing only to around 0.85 at \(k=5\), 0.78 at \(k=15\), and holding above 0.75 even when twenty global heads are removed. The dashed line at 0.89 denotes the unablated baseline.
      </p>
      <p>
        Global ablation exhibits a significantly slower decrease in zero-shot accuracy compared to local head ablation as heads are removed. The sharper performance degradation when ablating local heads demonstrates that ViT's CIFAR-10 classifier leans heavily on short-range, spatially local interactions. Even though ViTs are architecturally 'global,' their learned attention patterns include crucial local-neighborhood features.
      </p>
      <img src="figures/ablation_accuracy_CLIP_k25.png" alt="Ablation accuracy">
      <img src="figures/ablation_distance_CLIP_k25.png" alt="Average distance of ablated tokens">
      <p>
        Notably, the average distance of locally ablated tokens remains below 2.0 even when up to 25 heads are removed, whereas globally ablated tokens average over 3.5. This confirms that our local/global head selection effectively separates short-range from long-range attention.      </p>
    
      <h3>4.4 Analysis on a Single COCO Image and CLIP ViT-L/14 with 336 px patches</h3>
      <p>
        We analyze token-distance bias using a single COCO 2017 validation image at 336x336 resolution, processed by CLIP ViT-L/14 with 336 px patches (<code>model: openai/clip-vit-large-patch14-336</code>). For each of the 24 transformer layers, we plot the average self-attention weight as a function of normalized patch-to-patch distance (corner→corner normalized to 1).
      </p>
      <img src="figures/token_distance_bias_with_zoom_CLIP_COCO.png" alt="Zoomed token-distance bias curves">
      <p>
        The plots show a pronounced concentration of attention on very nearby patches. In the full view (top), nearly all layers place over 50% of their attention mass within the closest 5% of the image extent. The zoomed inset (bottom) reveals that attention weight decays by an order of magnitude by a normalized distance of ~0.1 and then flattens near zero beyond ~0.2.
      </p>
      <p>
        <strong>Layerwise patterns:</strong> Early layers (0-5) exhibit the steepest decay, with most weight on immediate neighbors (d < 0.05). Middle layers (6-15) show a more gradual fall-off, integrating slightly broader context while retaining locality. Even in the final layers (16-23), attention remains heavily skewed toward local patches, with distant-token weights remaining below 1-2% on average.
      </p>
      <p>
        <strong>Interpretation: </strong>Despite the global receptive field granted by self-attention, CLIP's ViT-L/14 backbone strongly biases attention toward nearby tokens at all depths. This emergent spatial locality suggests that - even on complex COCO imagery - local patch interactions form the backbone of the model's internal representations, with global context layered on top rather than relied upon exclusively.
      </p>
      <h4>4.1.1 Center vs. Edge Queries</h4>
      <p>
        We compare average attention-distance curves when the query patch is at the image center vs. a corner. Let
      </p>
      <div class="formula">
        $$\bar{A}_\text{center}(d),\quad \bar{A}_\text{corner}(d).$$
      </div>
      <img src="figures/center_corner_locality_CLIP.png" alt="Center vs. corner locality">
      <p>
        <strong>Interpretation:</strong> Center patches are more local: a strong peak at d≈0-0.2, then rapid fall-off.
          Corner patches have weaker local peak (fewer neighbors) and spikes at d≈0.65-0.75, corresponding to opposite edges.
      </p>
      <p>
        <strong>Implication:</strong> There is positional asymmetry in self-attention: center tokens concentrate on immediate vicinity, whereas border/corner tokens compensate by attending to specific mid-range or opposite patches. This boundary effect is baked into the model's geometry and could influence feature formation near image edges vs. interior.
      </p>
      
      <h4>4.1.2 Attention Rollout</h4>
      <p>
        We compute rollout heatmaps after k layers to visualize CLS's effective receptive field:
      </p>
      <img src="figures/attention_rollout_CLIP.png" alt="Attention rollout heatmap">
      <p>
        In this heatmap, bright yellow patches (≈1.0) are the strongest contributors to the final pooled feature, whereas dark purple patches (≈0.25) contribute minimally.        
      </p>
    </section>

    <section id="conclusion">
      <h2>5. Conclusion and Discussion</h2>
      <p>
        We explored token-level spatial locality in Vision Transformers by targeted attention masking and token ablation on CIFAR-10 using CLIP's ViT-base-patch-32 backbone. Our experiments quantified how constraining attention radii and removing spatially proximate tokens impact zero-shot CIFAR-10 classification accuracy. The results confirm that ViTs, while architecturally global, learn to exploit local-neighborhood interactions as primary information sources.
      </p>
      <p>
        <strong>Limitations:</strong> We evaluated only a single CLIP variant (openai/clip-vit-base-patch-32) and a single dataset (CIFAR-10). Limited compute restricted us to this prototype setting, and larger datasets or models may reveal different locality patterns. Attention masking may introduce artifacts, and token ablation assumes independent token contributions.
      </p>
      <p>
        <strong>Future Work:</strong> Extending this analysis to larger benchmarks (e.g., COCO, ImageNet) and other ViT variants (e.g., ViT-large, hierarchical hybrids) would test the generality of our findings. Incorporating activation patching for causal layer-specific interventions, exploring locality evolution during fine-tuning on diverse tasks (e.g., detection, segmentation), and developing locality-aware attention mechanisms could further improve efficiency and interpretability.
      </p>
      <p>
        Overall, our study demonstrates that implicit spatial locality emerges in Transformer attention maps, suggesting opportunities for hybrid architectures and more efficient, locality-aware designs.
      </p>
    </section>

    <section id="references">
      <h2>References</h2>
      <ol>
        <li>Dosovitskiy, A., et al. "An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale." <em>ICLR</em>, 2020.</li>
        <li>Caron, M., et al. "Emergent Locality in Vision Transformers." <em>CVPR</em>, 2023.</li>
        <li>Kazemied, P., et al. "Token Information Retention in Intermediate Layers of Vision Transformers." <em>NeurIPS</em>, 2024.</li>
        <li>LessWrong Community Contributors. "Emergent Localized Attention Patterns in Multimodal Audio Models." <em>LessWrong</em>, 2023.</li>
        <li>Abnar, S., & Zuidema, W. "Quantifying Attention Flow in Transformers." EMNLP 2020.</li>
        <li>Chefer, H., Gur, S., & Wolf, L. "Generic Attention-model Explainability..." arXiv:2103.15679, 2021.</li>
        <li>Radford, A., et al. "Learning Transferable Visual Models From Natural Language Supervision." arXiv:2103.00020, 2021.</li>
      </ol>
    </section>
  </article>
</body>
</html>
